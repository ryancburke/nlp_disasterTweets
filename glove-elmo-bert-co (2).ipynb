{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\n# Install java\n! apt-get update -qq\n! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\nos.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n! java -version\n\n# Install pyspark\n! pip install --ignore-installed -q pyspark==2.4.4\n! pip install --ignore-installed -q spark-nlp==2.7.1","execution_count":2,"outputs":[{"output_type":"stream","text":"debconf: delaying package configuration, since apt-utils is not installed\nopenjdk version \"1.8.0_282\"\nOpenJDK Runtime Environment (build 1.8.0_282-8u282-b08-0ubuntu1~18.04-b08)\nOpenJDK 64-Bit Server VM (build 25.282-b08, mixed mode)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sparknlp\n\nspark = sparknlp.start(gpu = True) # for GPU training >> sparknlp.start(gpu = True) # for Spark 2.3 =>> sparknlp.start(spark23 = True)\n\nfrom sparknlp.base import *\nfrom sparknlp.annotator import *\nfrom pyspark.ml import Pipeline\nimport pandas as pd\n\nprint(\"Spark NLP version\", sparknlp.version())\n\nprint(\"Apache Spark version:\", spark.version)\n\nspark","execution_count":3,"outputs":[{"output_type":"stream","text":"Spark NLP version 2.7.1\nApache Spark version: 2.4.4\n","name":"stdout"},{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"<pyspark.sql.session.SparkSession at 0x7f8a9ac202d0>","text/html":"\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://524508aaa889:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.4</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Spark NLP</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Read the training data "},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDataset = spark.read \\\n      .option(\"header\", True) \\\n      .csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n\ntrainDataset.show(truncate=50)","execution_count":4,"outputs":[{"output_type":"stream","text":"+---+-------+--------+--------------------------------------------------+------+\n| id|keyword|location|                                              text|target|\n+---+-------+--------+--------------------------------------------------+------+\n|  1|   null|    null|Our Deeds are the Reason of this #earthquake Ma...|     1|\n|  4|   null|    null|            Forest fire near La Ronge Sask. Canada|     1|\n|  5|   null|    null|All residents asked to 'shelter in place' are b...|     1|\n|  6|   null|    null|13,000 people receive #wildfires evacuation ord...|     1|\n|  7|   null|    null|Just got sent this photo from Ruby #Alaska as s...|     1|\n|  8|   null|    null|#RockyFire Update => California Hwy. 20 closed ...|     1|\n| 10|   null|    null|#flood #disaster Heavy rain causes flash floodi...|     1|\n| 13|   null|    null|I'm on top of the hill and I can see a fire in ...|     1|\n| 14|   null|    null|There's an emergency evacuation happening now i...|     1|\n| 15|   null|    null|I'm afraid that the tornado is coming to our ar...|     1|\n| 16|   null|    null|       Three people died from the heat wave so far|     1|\n| 17|   null|    null|Haha South Tampa is getting flooded hah- WAIT A...|     1|\n| 18|   null|    null|#raining #flooding #Florida #TampaBay #Tampa 18...|     1|\n| 19|   null|    null|           #Flood in Bago Myanmar #We arrived Bago|     1|\n| 20|   null|    null|Damage to school bus on 80 in multi car crash #...|     1|\n| 23|   null|    null|                                    What's up man?|     0|\n| 24|   null|    null|                                     I love fruits|     0|\n| 25|   null|    null|                                  Summer is lovely|     0|\n| 26|   null|    null|                                 My car is so fast|     0|\n| 28|   null|    null|                      What a goooooooaaaaaal!!!!!!|     0|\n+---+-------+--------+--------------------------------------------------+------+\nonly showing top 20 rows\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDataset.count()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"8387"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## identify cases of missing target "},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.functions import col\n\ntrainDataset.groupBy(\"target\") \\\n    .count() \\\n    .orderBy(col(\"count\").desc()) \\\n    .show()","execution_count":6,"outputs":[{"output_type":"stream","text":"+------+-----+\n|target|count|\n+------+-----+\n|     0| 4095|\n|     1| 3081|\n|  null| 1211|\n+------+-----+\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## check for text > 512 in length"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.functions import length\ntrainDataset.where(length(col(\"text\")) > 512).show()","execution_count":7,"outputs":[{"output_type":"stream","text":"+---+-------+--------+----+------+\n| id|keyword|location|text|target|\n+---+-------+--------+----+------+\n+---+-------+--------+----+------+\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## drop missing values from the text and target columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = trainDataset.dropna(subset=['text', 'target'])","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupBy(\"target\") \\\n    .count() \\\n    .orderBy(col(\"count\").desc()) \\\n    .show()","execution_count":9,"outputs":[{"output_type":"stream","text":"+------+-----+\n|target|count|\n+------+-----+\n|     0| 4095|\n|     1| 3081|\n+------+-----+\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## preprocessing pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"document_assembler = DocumentAssembler() \\\n.setInputCol(\"text\") \\\n.setOutputCol(\"document\") \\\n.setCleanupMode(\"shrink\")\n    \ntokenizer = Tokenizer() \\\n.setInputCols([\"document\"]) \\\n.setOutputCol(\"token\") \\\n.setSplitChars(['-']) \\\n.setContextChars(['(', ')', '?', '!', '#', '@']) \n\nnormalizer = Normalizer() \\\n.setInputCols([\"token\"]) \\\n.setOutputCol(\"normalized\")\\\n.setCleanupPatterns([\"[^\\w\\d\\s]\"]) \n\nstopwords_cleaner = StopWordsCleaner()\\\n.setInputCols(\"normalized\")\\\n.setOutputCol(\"cleanTokens\")\\\n.setCaseSensitive(False)\n\nlemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n.setInputCols([\"cleanTokens\"]) \\\n.setOutputCol(\"lemma\")\n\npreproc_pipeline = Pipeline(\n  stages=[document_assembler, \n            tokenizer,\n            normalizer,\n            stopwords_cleaner, \n            lemma])","execution_count":10,"outputs":[{"output_type":"stream","text":"lemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n[OK!]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## visualize results of the steps in pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipelineModel = preproc_pipeline.fit(train)\n%time result = pipelineModel.transform(train).collect()","execution_count":11,"outputs":[{"output_type":"stream","text":"CPU times: user 3.63 s, sys: 216 ms, total: 3.85 s\nWall time: 18.6 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql import Row\n\ntext = \"As she sat watching the world go by, something caught her eye. It wasn't so much its color or shape, but the way it was moving.\"\ndf = spark.createDataFrame(list(map(lambda x: Row(text=x), [text])), [\"text\"])\n%time result = pipelineModel.transform(df).collect()","execution_count":12,"outputs":[{"output_type":"stream","text":"CPU times: user 154 ms, sys: 24.6 ms, total: 178 ms\nWall time: 1.32 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n\npipelineModel = preproc_pipeline.fit(empty_df)\n","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sparknlp.base import LightPipeline\n\nlight_model = LightPipeline(pipelineModel)\n\n%time light_result = light_model.annotate(\"As she sat watching the world go by, something caught her eye. It wasn't so much its color or shape, but the way it was moving.\")","execution_count":14,"outputs":[{"output_type":"stream","text":"CPU times: user 15.3 ms, sys: 15.2 ms, total: 30.5 ms\nWall time: 60 ms\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"light_result.keys()","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"dict_keys(['lemma', 'document', 'normalized', 'cleanTokens', 'token'])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(light_result['token'], light_result['normalized'], light_result['cleanTokens'],  light_result['lemma']))","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"[('As', 'As', 'sat', 'sit'),\n ('she', 'she', 'watching', 'watch'),\n ('sat', 'sat', 'world', 'world'),\n ('watching', 'watching', 'go', 'go'),\n ('the', 'the', 'something', 'something'),\n ('world', 'world', 'caught', 'catch'),\n ('go', 'go', 'eye', 'eye'),\n ('by,', 'by', 'wasnt', 'wasnt'),\n ('something', 'something', 'much', 'much'),\n ('caught', 'caught', 'color', 'color'),\n ('her', 'her', 'shape', 'shape'),\n ('eye.', 'eye', 'way', 'way'),\n ('It', 'It', 'moving', 'move')]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Classification using GloVe"},{"metadata":{"trusted":true},"cell_type":"code","source":"document_assembler = DocumentAssembler() \\\n.setInputCol(\"text\") \\\n.setOutputCol(\"document\") \\\n.setCleanupMode(\"shrink\")\n    \ntokenizer = Tokenizer() \\\n.setInputCols([\"document\"]) \\\n.setOutputCol(\"token\") \\\n.setSplitChars(['-']) \\\n.setContextChars(['(', ')', '?', '!', '#', '@']) \n\nnormalizer = Normalizer() \\\n.setInputCols([\"token\"]) \\\n.setOutputCol(\"normalized\")\\\n.setCleanupPatterns([\"[^\\w\\d\\s]\"]) \n\nstopwords_cleaner = StopWordsCleaner()\\\n.setInputCols(\"normalized\")\\\n.setOutputCol(\"cleanTokens\")\\\n.setCaseSensitive(False)\n\nlemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n.setInputCols([\"cleanTokens\"]) \\\n.setOutputCol(\"lemma\")\n\nglove_embeddings = WordEmbeddingsModel().pretrained() \\\n.setInputCols([\"document\",'lemma'])\\\n.setOutputCol(\"embeddings\")\\\n.setCaseSensitive(False)\n\nembeddingsSentence = SentenceEmbeddings() \\\n.setInputCols([\"document\", \"embeddings\"]) \\\n.setOutputCol(\"sentence_embeddings\") \\\n.setPoolingStrategy(\"AVERAGE\")\n\nclasssifierdl = ClassifierDLApproach()\\\n.setInputCols([\"sentence_embeddings\"])\\\n.setOutputCol(\"class\")\\\n.setLabelColumn(\"target\")\\\n.setMaxEpochs(5)\\\n.setLr(0.001)\\\n.setBatchSize(8)\\\n.setEnableOutputLogs(True)\n#.setOutputLogsPath('logs')\n\nglove_clf_pipeline = Pipeline(\n    stages=[document_assembler, \n            tokenizer,\n            normalizer,\n            stopwords_cleaner, \n            lemma, \n            glove_embeddings,\n            embeddingsSentence,\n            classsifierdl])","execution_count":17,"outputs":[{"output_type":"stream","text":"lemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n[OK!]\nglove_100d download started this may take some time.\nApproximate size to download 145.3 MB\n[OK!]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## split training data into train/validation sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"(train_df, val_df) = train.randomSplit([0.7, 0.3], seed = 8)\nprint(\"Training Dataset Count: \" + str(train_df.count()))\nprint(\"Validation Dataset Count: \" + str(val_df.count()))","execution_count":18,"outputs":[{"output_type":"stream","text":"Training Dataset Count: 5036\nValidation Dataset Count: 2140\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_clf_pipelineModel = glove_clf_pipeline.fit(train_df)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cd ~/annotator_logs && ls -l","execution_count":20,"outputs":[{"output_type":"stream","text":"total 4\r\n-rw-r--r-- 1 root root 449 Mar 16 09:22 ClassifierDLApproach_af6576cde5d2.log\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat ~/annotator_logs/ClassifierDLApproach_b5fa4fd51592.log","execution_count":21,"outputs":[{"output_type":"stream","text":"cat: /root/annotator_logs/ClassifierDLApproach_b5fa4fd51592.log: No such file or directory\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the predictions on validation Set\n\npreds = glove_clf_pipelineModel.transform(val_df)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds.select('text','target',\"class.result\").show(10, truncate=80)","execution_count":23,"outputs":[{"output_type":"stream","text":"+--------------------------------------------------------------------------------+------+------+\n|                                                                            text|target|result|\n+--------------------------------------------------------------------------------+------+------+\n|http://t.co/GKYe6gjTk5 Had a #personalinjury accident this summer? Read our a...|     0|   [0]|\n|Meet Brinco your own personal earthquake snd tsunami early warning beacon. ht...|     1|   [1]|\n|                                                    I want some tsunami take out|     0|   [1]|\n|Just stop fucking saying ÛÏa whole Û÷notherÛ. It just sounds fucking stup...|     0|   [0]|\n|Brain twister homefolks are opinionated over against proposal modernized cana...|     0|   [0]|\n|Crazy Mom Threw Teen Daughter a NUDE Twister Sex Party According To Her Frien...|     0|   [0]|\n|     The Sharper Image Viper 24' Hardside Twister (Black) http://t.co/FXk3zsj2PE|     0|   [0]|\n|                         Brain twister let drop up telly structuring cast: EDcXO|     0|   [0]|\n|                            @sarahmcpants @JustJon I'll give him a titty twister|     0|   [0]|\n|Reasons brain twister oneself should discount redesigning yours website: ItrA...|     0|   [0]|\n+--------------------------------------------------------------------------------+------+------+\nonly showing top 10 rows\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_df = preds.select('text','target',\"class.result\").toPandas()\n\n# The result is an array since in Spark NLP you can have multiple sentences.\n# Let's explode the array and get the item(s) inside of result column out\npreds_df['result'] = preds_df['result'].apply(lambda x : x[0])","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are going to use sklearn to evalute the results on test dataset\nfrom sklearn.metrics import classification_report\n\nprint (classification_report(preds_df['result'], preds_df['target']))","execution_count":25,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.85      0.81      0.83      1276\n           1       0.73      0.78      0.76       864\n\n    accuracy                           0.80      2140\n   macro avg       0.79      0.79      0.79      2140\nweighted avg       0.80      0.80      0.80      2140\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Classification using Elmo\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"document_assembler = DocumentAssembler() \\\n.setInputCol(\"text\") \\\n.setOutputCol(\"document\") \\\n.setCleanupMode(\"shrink\")\n    \ntokenizer = Tokenizer() \\\n.setInputCols([\"document\"]) \\\n.setOutputCol(\"token\") \\\n.setSplitChars(['-']) \\\n.setContextChars(['(', ')', '?', '!', '#', '@']) \n\nnormalizer = Normalizer() \\\n.setInputCols([\"token\"]) \\\n.setOutputCol(\"normalized\")\\\n.setCleanupPatterns([\"[^\\w\\d\\s]\"]) \n\nstopwords_cleaner = StopWordsCleaner()\\\n.setInputCols(\"normalized\")\\\n.setOutputCol(\"cleanTokens\")\\\n.setCaseSensitive(False)\n\nlemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n.setInputCols([\"cleanTokens\"]) \\\n.setOutputCol(\"lemma\")\n    \nelmo_embeddings = ElmoEmbeddings.pretrained('elmo')\\\n.setInputCols([\"document\", \"token\"])\\\n.setOutputCol(\"embeddings\")\\\n.setPoolingLayer('elmo')# default --> elmo\n\nembeddingsSentence = SentenceEmbeddings() \\\n.setInputCols([\"document\", \"embeddings\"]) \\\n.setOutputCol(\"sentence_embeddings\") \\\n.setPoolingStrategy(\"AVERAGE\")\n\nclasssifierdl = ClassifierDLApproach()\\\n.setInputCols([\"sentence_embeddings\"])\\\n.setOutputCol(\"class\")\\\n.setLabelColumn(\"target\")\\\n.setMaxEpochs(5)\\\n.setLr(0.001)\\\n.setBatchSize(8)\\\n.setEnableOutputLogs(True)\n#.setOutputLogsPath('logs')\n\nelmo_clf_pipeline = Pipeline(\n    stages=[document_assembler, \n            tokenizer,\n            normalizer,\n            stopwords_cleaner,\n            lemma,\n            elmo_embeddings,\n            embeddingsSentence,\n            classsifierdl])","execution_count":26,"outputs":[{"output_type":"stream","text":"lemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n[OK!]\nelmo download started this may take some time.\nApproximate size to download 334.1 MB\n[OK!]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"elmo_clf_pipelineModel = elmo_clf_pipeline.fit(train_df)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cd /root/annotator_logs && ls -lt","execution_count":28,"outputs":[{"output_type":"stream","text":"total 8\r\n-rw-r--r-- 1 root root 444 Mar 16 09:59 ClassifierDLApproach_90842185ddd7.log\r\n-rw-r--r-- 1 root root 449 Mar 16 09:22 ClassifierDLApproach_af6576cde5d2.log\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat /root/annotator_logs/ClassifierDLApproach_72aceb5a2cde.log","execution_count":29,"outputs":[{"output_type":"stream","text":"cat: /root/annotator_logs/ClassifierDLApproach_72aceb5a2cde.log: No such file or directory\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = elmo_clf_pipelineModel.transform(val_df)","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds.select('text','target',\"class.result\").show(10, truncate=80)","execution_count":31,"outputs":[{"output_type":"stream","text":"+--------------------------------------------------------------------------------+------+------+\n|                                                                            text|target|result|\n+--------------------------------------------------------------------------------+------+------+\n|http://t.co/GKYe6gjTk5 Had a #personalinjury accident this summer? Read our a...|     0|   [0]|\n|Meet Brinco your own personal earthquake snd tsunami early warning beacon. ht...|     1|   [1]|\n|                                                    I want some tsunami take out|     0|   [0]|\n|Just stop fucking saying ÛÏa whole Û÷notherÛ. It just sounds fucking stup...|     0|   [0]|\n|Brain twister homefolks are opinionated over against proposal modernized cana...|     0|   [0]|\n|Crazy Mom Threw Teen Daughter a NUDE Twister Sex Party According To Her Frien...|     0|   [0]|\n|     The Sharper Image Viper 24' Hardside Twister (Black) http://t.co/FXk3zsj2PE|     0|   [0]|\n|                         Brain twister let drop up telly structuring cast: EDcXO|     0|   [0]|\n|                            @sarahmcpants @JustJon I'll give him a titty twister|     0|   [0]|\n|Reasons brain twister oneself should discount redesigning yours website: ItrA...|     0|   [0]|\n+--------------------------------------------------------------------------------+------+------+\nonly showing top 10 rows\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are going to use sklearn to evalute the results on test dataset\nfrom sklearn.metrics import classification_report\n\n\n\npreds_df = preds.select('text','target',\"class.result\").toPandas()\n\npreds_df['result'] = preds_df['result'].apply(lambda x : x[0])\n\nprint (classification_report(preds_df['result'], preds_df['target']))","execution_count":32,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.90      0.81      0.85      1362\n           1       0.72      0.85      0.78       778\n\n    accuracy                           0.82      2140\n   macro avg       0.81      0.83      0.81      2140\nweighted avg       0.84      0.82      0.83      2140\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Classification using BERT\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"document_assembler = DocumentAssembler() \\\n.setInputCol(\"text\") \\\n.setOutputCol(\"document\") \\\n.setCleanupMode(\"shrink\")\n    \ntokenizer = Tokenizer() \\\n.setInputCols([\"document\"]) \\\n.setOutputCol(\"token\") \\\n.setSplitChars(['-']) \\\n.setContextChars(['(', ')', '?', '!', '#', '@']) \n\nnormalizer = Normalizer() \\\n.setInputCols([\"token\"]) \\\n.setOutputCol(\"normalized\")\\\n.setCleanupPatterns([\"[^\\w\\d\\s]\"]) \n\nstopwords_cleaner = StopWordsCleaner()\\\n.setInputCols(\"normalized\")\\\n.setOutputCol(\"cleanTokens\")\\\n.setCaseSensitive(False)\n\nlemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n.setInputCols([\"cleanTokens\"]) \\\n.setOutputCol(\"lemma\")\n\nbert_embeddings = BertEmbeddings().pretrained(name='bert_base_cased', lang='en') \\\n.setInputCols([\"document\",'token'])\\\n.setOutputCol(\"embeddings\")\n\nembeddingsSentence = SentenceEmbeddings() \\\n.setInputCols([\"document\", \"embeddings\"]) \\\n.setOutputCol(\"sentence_embeddings\") \\\n.setPoolingStrategy(\"AVERAGE\")\n\nclasssifierdl = ClassifierDLApproach()\\\n.setInputCols([\"sentence_embeddings\"])\\\n.setOutputCol(\"class\")\\\n.setLabelColumn(\"target\")\\\n.setMaxEpochs(5)\\\n.setLr(0.001)\\\n.setBatchSize(8) \\\n.setEnableOutputLogs(True)\n#.setOutputLogsPath('logs')\n\nbert_clf_pipeline = Pipeline(\n    stages=[document_assembler, \n            tokenizer,\n            normalizer,\n            stopwords_cleaner,\n            lemma,\n            bert_embeddings,\n            embeddingsSentence,\n            classsifierdl])","execution_count":33,"outputs":[{"output_type":"stream","text":"lemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n[OK!]\nbert_base_cased download started this may take some time.\nApproximate size to download 389.1 MB\n[OK!]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_clf_pipelineModel = bert_clf_pipeline.fit(train_df)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cd /root/annotator_logs && ls -lt","execution_count":35,"outputs":[{"output_type":"stream","text":"total 12\r\n-rw-r--r-- 1 root root 443 Mar 16 10:25 ClassifierDLApproach_c60cf9d00056.log\r\n-rw-r--r-- 1 root root 444 Mar 16 09:59 ClassifierDLApproach_90842185ddd7.log\r\n-rw-r--r-- 1 root root 449 Mar 16 09:22 ClassifierDLApproach_af6576cde5d2.log\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat /root/annotator_logs/ClassifierDLApproach_5571ae93049a.log","execution_count":36,"outputs":[{"output_type":"stream","text":"cat: /root/annotator_logs/ClassifierDLApproach_5571ae93049a.log: No such file or directory\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = bert_clf_pipelineModel.transform(val_df)","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds.select('text','target',\"class.result\").show(10, truncate=80)","execution_count":38,"outputs":[{"output_type":"stream","text":"+--------------------------------------------------------------------------------+------+------+\n|                                                                            text|target|result|\n+--------------------------------------------------------------------------------+------+------+\n|http://t.co/GKYe6gjTk5 Had a #personalinjury accident this summer? Read our a...|     0|   [0]|\n|Meet Brinco your own personal earthquake snd tsunami early warning beacon. ht...|     1|   [0]|\n|                                                    I want some tsunami take out|     0|   [0]|\n|Just stop fucking saying ÛÏa whole Û÷notherÛ. It just sounds fucking stup...|     0|   [0]|\n|Brain twister homefolks are opinionated over against proposal modernized cana...|     0|   [0]|\n|Crazy Mom Threw Teen Daughter a NUDE Twister Sex Party According To Her Frien...|     0|   [0]|\n|     The Sharper Image Viper 24' Hardside Twister (Black) http://t.co/FXk3zsj2PE|     0|   [0]|\n|                         Brain twister let drop up telly structuring cast: EDcXO|     0|   [0]|\n|                            @sarahmcpants @JustJon I'll give him a titty twister|     0|   [0]|\n|Reasons brain twister oneself should discount redesigning yours website: ItrA...|     0|   [0]|\n+--------------------------------------------------------------------------------+------+------+\nonly showing top 10 rows\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are going to use sklearn to evalute the results on test dataset\nfrom sklearn.metrics import classification_report\n\n\n\npreds_df = preds.select('text','target',\"class.result\").toPandas()\n\npreds_df['result'] = preds_df['result'].apply(lambda x : x[0])\n\nprint (classification_report(preds_df['result'], preds_df['target']))","execution_count":39,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.93      0.77      0.85      1466\n           1       0.64      0.88      0.74       674\n\n    accuracy                           0.81      2140\n   macro avg       0.79      0.83      0.79      2140\nweighted avg       0.84      0.81      0.81      2140\n\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}