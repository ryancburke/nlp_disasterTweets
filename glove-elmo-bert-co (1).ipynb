{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\n# Install java\n! apt-get update -qq\n! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\nos.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n! java -version\n\n# Install pyspark\n! pip install --ignore-installed -q pyspark==2.4.4\n! pip install --ignore-installed -q spark-nlp==2.7.1","execution_count":2,"outputs":[{"output_type":"stream","text":"debconf: delaying package configuration, since apt-utils is not installed\nopenjdk version \"1.8.0_282\"\nOpenJDK Runtime Environment (build 1.8.0_282-8u282-b08-0ubuntu1~18.04-b08)\nOpenJDK 64-Bit Server VM (build 25.282-b08, mixed mode)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sparknlp\n\nspark = sparknlp.start(gpu = True) # for GPU training >> sparknlp.start(gpu = True) # for Spark 2.3 =>> sparknlp.start(spark23 = True)\n\nfrom sparknlp.base import *\nfrom sparknlp.annotator import *\nfrom pyspark.ml import Pipeline\nimport pandas as pd\n\nprint(\"Spark NLP version\", sparknlp.version())\n\nprint(\"Apache Spark version:\", spark.version)\n\nspark","execution_count":3,"outputs":[{"output_type":"stream","text":"Spark NLP version 2.7.1\nApache Spark version: 2.4.4\n","name":"stdout"},{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"<pyspark.sql.session.SparkSession at 0x7f4e0cc561d0>","text/html":"\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://93129efca2c3:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.4</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Spark NLP</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Read the training data "},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDataset = spark.read \\\n      .option(\"header\", True) \\\n      .csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n\ntrainDataset.show(truncate=50)","execution_count":4,"outputs":[{"output_type":"stream","text":"+---+-------+--------+--------------------------------------------------+------+\n| id|keyword|location|                                              text|target|\n+---+-------+--------+--------------------------------------------------+------+\n|  1|   null|    null|Our Deeds are the Reason of this #earthquake Ma...|     1|\n|  4|   null|    null|            Forest fire near La Ronge Sask. Canada|     1|\n|  5|   null|    null|All residents asked to 'shelter in place' are b...|     1|\n|  6|   null|    null|13,000 people receive #wildfires evacuation ord...|     1|\n|  7|   null|    null|Just got sent this photo from Ruby #Alaska as s...|     1|\n|  8|   null|    null|#RockyFire Update => California Hwy. 20 closed ...|     1|\n| 10|   null|    null|#flood #disaster Heavy rain causes flash floodi...|     1|\n| 13|   null|    null|I'm on top of the hill and I can see a fire in ...|     1|\n| 14|   null|    null|There's an emergency evacuation happening now i...|     1|\n| 15|   null|    null|I'm afraid that the tornado is coming to our ar...|     1|\n| 16|   null|    null|       Three people died from the heat wave so far|     1|\n| 17|   null|    null|Haha South Tampa is getting flooded hah- WAIT A...|     1|\n| 18|   null|    null|#raining #flooding #Florida #TampaBay #Tampa 18...|     1|\n| 19|   null|    null|           #Flood in Bago Myanmar #We arrived Bago|     1|\n| 20|   null|    null|Damage to school bus on 80 in multi car crash #...|     1|\n| 23|   null|    null|                                    What's up man?|     0|\n| 24|   null|    null|                                     I love fruits|     0|\n| 25|   null|    null|                                  Summer is lovely|     0|\n| 26|   null|    null|                                 My car is so fast|     0|\n| 28|   null|    null|                      What a goooooooaaaaaal!!!!!!|     0|\n+---+-------+--------+--------------------------------------------------+------+\nonly showing top 20 rows\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDataset.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## identify cases of missing target "},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.functions import col\n\ntrainDataset.groupBy(\"target\") \\\n    .count() \\\n    .orderBy(col(\"count\").desc()) \\\n    .show()","execution_count":5,"outputs":[{"output_type":"stream","text":"+------+-----+\n|target|count|\n+------+-----+\n|     0| 4095|\n|     1| 3081|\n|  null| 1211|\n+------+-----+\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## check for text > 512 in length"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.functions import length\ntrainDataset.where(length(col(\"text\")) > 512).show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## drop missing values from the text and target columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = trainDataset.dropna(subset=['text', 'target'])","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupBy(\"target\") \\\n    .count() \\\n    .orderBy(col(\"count\").desc()) \\\n    .show()","execution_count":7,"outputs":[{"output_type":"stream","text":"+------+-----+\n|target|count|\n+------+-----+\n|     0| 4095|\n|     1| 3081|\n+------+-----+\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## preprocessing pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"document_assembler = DocumentAssembler() \\\n.setInputCol(\"text\") \\\n.setOutputCol(\"document\") \\\n.setCleanupMode(\"shrink\")\n    \ntokenizer = Tokenizer() \\\n.setInputCols([\"document\"]) \\\n.setOutputCol(\"token\") \\\n.setSplitChars(['-']) \\\n.setContextChars(['(', ')', '?', '!', '#', '@']) \n\nnormalizer = Normalizer() \\\n.setInputCols([\"token\"]) \\\n.setOutputCol(\"normalized\")\\\n.setCleanupPatterns([\"[^\\w\\d\\s]\"]) \n\nstopwords_cleaner = StopWordsCleaner()\\\n.setInputCols(\"normalized\")\\\n.setOutputCol(\"cleanTokens\")\\\n.setCaseSensitive(False)\n\nlemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n.setInputCols([\"cleanTokens\"]) \\\n.setOutputCol(\"lemma\")\n\npreproc_pipeline = Pipeline(\n  stages=[document_assembler, \n            tokenizer,\n            normalizer,\n            stopwords_cleaner, \n            lemma])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## visualize results of the steps in pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipelineModel = preproc_pipeline.fit(train)\n%time result = pipelineModel.transform(train).collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql import Row\n\ntext = \"As she sat watching the world go by, something caught her eye. It wasn't so much its color or shape, but the way it was moving.\"\ndf = spark.createDataFrame(list(map(lambda x: Row(text=x), [text])), [\"text\"])\n%time result = pipelineModel.transform(df).collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n\npipelineModel = preproc_pipeline.fit(empty_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sparknlp.base import LightPipeline\n\nlight_model = LightPipeline(pipelineModel)\n\n%time light_result = light_model.annotate(\"As she sat watching the world go by, something caught her eye. It wasn't so much its color or shape, but the way it was moving.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"light_result.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(light_result['token'], light_result['normalized'], light_result['cleanTokens'],  light_result['lemma']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification using GloVe"},{"metadata":{"trusted":true},"cell_type":"code","source":"document_assembler = DocumentAssembler() \\\n.setInputCol(\"text\") \\\n.setOutputCol(\"document\") \\\n.setCleanupMode(\"shrink\")\n    \ntokenizer = Tokenizer() \\\n.setInputCols([\"document\"]) \\\n.setOutputCol(\"token\") \\\n.setSplitChars(['-']) \\\n.setContextChars(['(', ')', '?', '!', '#', '@']) \n\nnormalizer = Normalizer() \\\n.setInputCols([\"token\"]) \\\n.setOutputCol(\"normalized\")\\\n.setCleanupPatterns([\"[^\\w\\d\\s]\"]) \n\nstopwords_cleaner = StopWordsCleaner()\\\n.setInputCols(\"normalized\")\\\n.setOutputCol(\"cleanTokens\")\\\n.setCaseSensitive(False)\n\nlemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n.setInputCols([\"cleanTokens\"]) \\\n.setOutputCol(\"lemma\")\n\nglove_embeddings = WordEmbeddingsModel().pretrained() \\\n.setInputCols([\"document\",'lemma'])\\\n.setOutputCol(\"embeddings\")\\\n.setCaseSensitive(False)\n\nembeddingsSentence = SentenceEmbeddings() \\\n.setInputCols([\"document\", \"embeddings\"]) \\\n.setOutputCol(\"sentence_embeddings\") \\\n.setPoolingStrategy(\"AVERAGE\")\n\nclasssifierdl = ClassifierDLApproach()\\\n.setInputCols([\"sentence_embeddings\"])\\\n.setOutputCol(\"class\")\\\n.setLabelColumn(\"target\")\\\n.setMaxEpochs(5)\\\n.setLr(0.001)\\\n.setBatchSize(8)\\\n.setEnableOutputLogs(True)\n#.setOutputLogsPath('logs')\n\nglove_clf_pipeline = Pipeline(\n    stages=[document_assembler, \n            tokenizer,\n            normalizer,\n            stopwords_cleaner, \n            lemma, \n            glove_embeddings,\n            embeddingsSentence,\n            classsifierdl])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## split training data into train/validation sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"(train_df, val_df) = train.randomSplit([0.7, 0.3], seed = 8)\nprint(\"Training Dataset Count: \" + str(train_df.count()))\nprint(\"Validation Dataset Count: \" + str(val_df.count()))","execution_count":8,"outputs":[{"output_type":"stream","text":"Training Dataset Count: 5036\nValidation Dataset Count: 2140\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_clf_pipelineModel = glove_clf_pipeline.fit(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cd ~/annotator_logs && ls -l","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat ~/annotator_logs/ClassifierDLApproach_b5fa4fd51592.log","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the predictions on validation Set\n\npreds = glove_clf_pipelineModel.transform(val_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds.select('text','target',\"class.result\").show(10, truncate=80)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_df = preds.select('text','target',\"class.result\").toPandas()\n\n# The result is an array since in Spark NLP you can have multiple sentences.\n# Let's explode the array and get the item(s) inside of result column out\npreds_df['result'] = preds_df['result'].apply(lambda x : x[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are going to use sklearn to evalute the results on test dataset\nfrom sklearn.metrics import classification_report\n\nprint (classification_report(preds_df['result'], preds_df['target']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification using Elmo\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"document_assembler = DocumentAssembler() \\\n.setInputCol(\"text\") \\\n.setOutputCol(\"document\") \\\n.setCleanupMode(\"shrink\")\n    \ntokenizer = Tokenizer() \\\n.setInputCols([\"document\"]) \\\n.setOutputCol(\"token\") \\\n.setSplitChars(['-']) \\\n.setContextChars(['(', ')', '?', '!', '#', '@']) \n\nnormalizer = Normalizer() \\\n.setInputCols([\"token\"]) \\\n.setOutputCol(\"normalized\")\\\n.setCleanupPatterns([\"[^\\w\\d\\s]\"]) \n\nstopwords_cleaner = StopWordsCleaner()\\\n.setInputCols(\"normalized\")\\\n.setOutputCol(\"cleanTokens\")\\\n.setCaseSensitive(False)\n\nlemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n.setInputCols([\"cleanTokens\"]) \\\n.setOutputCol(\"lemma\")\n    \nelmo_embeddings = ElmoEmbeddings.pretrained('elmo')\\\n.setInputCols([\"document\", \"token\"])\\\n.setOutputCol(\"embeddings\")\\\n.setPoolingLayer('elmo')# default --> elmo\n\nembeddingsSentence = SentenceEmbeddings() \\\n.setInputCols([\"document\", \"embeddings\"]) \\\n.setOutputCol(\"sentence_embeddings\") \\\n.setPoolingStrategy(\"AVERAGE\")\n\nclasssifierdl = ClassifierDLApproach()\\\n.setInputCols([\"sentence_embeddings\"])\\\n.setOutputCol(\"class\")\\\n.setLabelColumn(\"target\")\\\n.setMaxEpochs(5)\\\n.setLr(0.001)\\\n.setBatchSize(8)\\\n.setEnableOutputLogs(True)\n#.setOutputLogsPath('logs')\n\nelmo_clf_pipeline = Pipeline(\n    stages=[document_assembler, \n            tokenizer,\n            normalizer,\n            stopwords_cleaner,\n            lemma,\n            elmo_embeddings,\n            embeddingsSentence,\n            classsifierdl])","execution_count":9,"outputs":[{"output_type":"stream","text":"lemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n[OK!]\nelmo download started this may take some time.\nApproximate size to download 334.1 MB\n[OK!]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"elmo_clf_pipelineModel = elmo_clf_pipeline.fit(train_df)","execution_count":10,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-a7e792634f18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0melmo_clf_pipelineModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melmo_clf_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cd /root/annotator_logs && ls -lt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat /root/annotator_logs/ClassifierDLApproach_72aceb5a2cde.log","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = elmo_clf_pipelineModel.transform(val_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds.select('text','target',\"class.result\").show(10, truncate=80)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are going to use sklearn to evalute the results on test dataset\nfrom sklearn.metrics import classification_report\n\n\n\npreds_df = preds.select('text','target',\"class.result\").toPandas()\n\npreds_df['result'] = preds_df['result'].apply(lambda x : x[0])\n\nprint (classification_report(preds_df['result'], preds_df['target']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification using BERT\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"document_assembler = DocumentAssembler() \\\n.setInputCol(\"text\") \\\n.setOutputCol(\"document\") \\\n.setCleanupMode(\"shrink\")\n    \ntokenizer = Tokenizer() \\\n.setInputCols([\"document\"]) \\\n.setOutputCol(\"token\") \\\n.setSplitChars(['-']) \\\n.setContextChars(['(', ')', '?', '!', '#', '@']) \n\nnormalizer = Normalizer() \\\n.setInputCols([\"token\"]) \\\n.setOutputCol(\"normalized\")\\\n.setCleanupPatterns([\"[^\\w\\d\\s]\"]) \n\nstopwords_cleaner = StopWordsCleaner()\\\n.setInputCols(\"normalized\")\\\n.setOutputCol(\"cleanTokens\")\\\n.setCaseSensitive(False)\n\nlemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n.setInputCols([\"cleanTokens\"]) \\\n.setOutputCol(\"lemma\")\n\nbert_embeddings = BertEmbeddings().pretrained(name='bert_base_cased', lang='en') \\\n.setInputCols([\"document\",'token'])\\\n.setOutputCol(\"embeddings\")\n\nembeddingsSentence = SentenceEmbeddings() \\\n.setInputCols([\"document\", \"embeddings\"]) \\\n.setOutputCol(\"sentence_embeddings\") \\\n.setPoolingStrategy(\"AVERAGE\")\n\nclasssifierdl = ClassifierDLApproach()\\\n.setInputCols([\"sentence_embeddings\"])\\\n.setOutputCol(\"class\")\\\n.setLabelColumn(\"target\")\\\n.setMaxEpochs(5)\\\n.setLr(0.001)\\\n.setBatchSize(8) \\\n.setEnableOutputLogs(True)\n#.setOutputLogsPath('logs')\n\nbert_clf_pipeline = Pipeline(\n    stages=[document_assembler, \n            tokenizer,\n            normalizer,\n            stopwords_cleaner,\n            lemma,\n            bert_embeddings,\n            embeddingsSentence,\n            classsifierdl])","execution_count":18,"outputs":[{"output_type":"stream","text":"lemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n[OK!]\nbert_base_cased download started this may take some time.\nApproximate size to download 389.1 MB\n[OK!]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_clf_pipelineModel = bert_clf_pipeline.fit(train_df)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cd /root/annotator_logs && ls -lt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat /root/annotator_logs/ClassifierDLApproach_5571ae93049a.log","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = bert_clf_pipelineModel.transform(val_df)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds.select('text','target',\"class.result\").show(10, truncate=80)","execution_count":16,"outputs":[{"output_type":"stream","text":"+--------------------------------------------------------------------------------+------+------+\n|                                                                            text|target|result|\n+--------------------------------------------------------------------------------+------+------+\n|http://t.co/GKYe6gjTk5 Had a #personalinjury accident this summer? Read our a...|     0|   [0]|\n|Meet Brinco your own personal earthquake snd tsunami early warning beacon. ht...|     1|   [0]|\n|                                                    I want some tsunami take out|     0|   [0]|\n|Just stop fucking saying ÛÏa whole Û÷notherÛ. It just sounds fucking stup...|     0|   [0]|\n|Brain twister homefolks are opinionated over against proposal modernized cana...|     0|   [0]|\n|Crazy Mom Threw Teen Daughter a NUDE Twister Sex Party According To Her Frien...|     0|   [0]|\n|     The Sharper Image Viper 24' Hardside Twister (Black) http://t.co/FXk3zsj2PE|     0|   [0]|\n|                         Brain twister let drop up telly structuring cast: EDcXO|     0|   [0]|\n|                            @sarahmcpants @JustJon I'll give him a titty twister|     0|   [0]|\n|Reasons brain twister oneself should discount redesigning yours website: ItrA...|     0|   [0]|\n+--------------------------------------------------------------------------------+------+------+\nonly showing top 10 rows\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are going to use sklearn to evalute the results on test dataset\nfrom sklearn.metrics import classification_report\n\n\n\npreds_df = preds.select('text','target',\"class.result\").toPandas()\n\npreds_df['result'] = preds_df['result'].apply(lambda x : x[0])\n\nprint (classification_report(preds_df['result'], preds_df['target']))","execution_count":17,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       1.00      0.57      0.72      2140\n           1       0.00      0.00      0.00         0\n\n    accuracy                           0.57      2140\n   macro avg       0.50      0.28      0.36      2140\nweighted avg       1.00      0.57      0.72      2140\n\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"## ClassifierDL with universal sentence embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# actual content is inside text column\ndocument = DocumentAssembler()\\\n    .setInputCol(\"text\")\\\n    .setOutputCol(\"document\")\n    \n# we can also use sentece detector here if we want to train on and get predictions for each sentence\n\nuse = UniversalSentenceEncoder.pretrained(\"tfhub_use\", \"en\") \\\n      .setInputCols(\"document\") \\\n      .setOutputCol(\"sentence_embeddings\")\n\n# the classes/labels/categories are in category column\nclasssifierdl = ClassifierDLApproach()\\\n  .setInputCols([\"sentence_embeddings\"])\\\n  .setOutputCol(\"class\")\\\n  .setLabelColumn(\"target\")\\\n  .setMaxEpochs(5)\\\n  .setLr(0.001)\\\n  .setBatchSize(8)\\\n  .setEnableOutputLogs(True)\n\nuse_clf_pipeline = Pipeline(\n    stages = [\n        document,\n        use,\n        classsifierdl\n    ])","execution_count":46,"outputs":[{"output_type":"stream","text":"tfhub_use download started this may take some time.\nApproximate size to download 923.7 MB\n[OK!]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"use_pipelineModel = use_clf_pipeline.fit(train_df)","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cd /root/annotator_logs && ls -lt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat ~/annotator_logs/ClassifierDLApproach_6b6f3f75388e.log","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are going to use sklearn to evalute the results on test dataset\nfrom sklearn.metrics import classification_report\n\npreds = use_pipelineModel.transform(val_df)\n\npreds_df = preds.select('text','target',\"class.result\").toPandas()\n\npreds_df['result'] = preds_df['result'].apply(lambda x : x[0])\n\nprint (classification_report(preds_df['result'], preds_df['target']))","execution_count":48,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.88      0.81      0.85      1322\n           1       0.73      0.83      0.78       818\n\n    accuracy                           0.82      2140\n   macro avg       0.81      0.82      0.81      2140\nweighted avg       0.83      0.82      0.82      2140\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## With BERT sentence embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# actual content is inside description column\ndocument = DocumentAssembler()\\\n    .setInputCol(\"text\")\\\n    .setOutputCol(\"document\")\n    \n# we can also use sentece detector here if we want to train on and get predictions for each sentence\n#bert_sent = BertSentenceEmbeddings.pretrained('sent_small_bert_L8_512')\\\nbert_sent = BertSentenceEmbeddings.pretrained('sent_bert_base_cased')\\\n .setInputCols([\"document\"])\\\n .setOutputCol(\"sentence_embeddings\")\n\n# the classes/labels/categories are in category column\nclasssifierdl = ClassifierDLApproach()\\\n  .setInputCols([\"sentence_embeddings\"])\\\n  .setOutputCol(\"class\")\\\n  .setLabelColumn(\"target\")\\\n  .setMaxEpochs(5)\\\n  .setLr(0.001)\\\n  .setBatchSize(8)\\\n  .setEnableOutputLogs(True)\n\nbert_sent_clf_pipeline = Pipeline(\n    stages = [\n        document,\n        bert_sent,\n        classsifierdl\n    ])","execution_count":49,"outputs":[{"output_type":"stream","text":"sent_bert_base_cased download started this may take some time.\nApproximate size to download 389.1 MB\n[OK!]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_sent_pipelineModel = bert_sent_clf_pipeline.fit(train_df)","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cd /root/annotator_logs && ls -lt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat ~/annotator_logs/ClassifierDLApproach_29562c9c6757.log","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are going to use sklearn to evalute the results on test dataset\nfrom sklearn.metrics import classification_report\n\npreds = bert_sent_pipelineModel.transform(val_df)\n\npreds_df = preds.select('text','target',\"class.result\").toPandas()\n\npreds_df['result'] = preds_df['result'].apply(lambda x : x[0])\n\nprint (classification_report(preds_df['result'], preds_df['target']))","execution_count":52,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.87      0.82      0.84      1303\n           1       0.74      0.82      0.78       837\n\n    accuracy                           0.82      2140\n   macro avg       0.81      0.82      0.81      2140\nweighted avg       0.82      0.82      0.82      2140\n\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}